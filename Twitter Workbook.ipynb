{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Workbook\n",
    "\n",
    "# Twitter Sentiment Analysis\n",
    "\n",
    "This notebook aims to predict whether a person does or does not intend to get the COVID-19 vaccine by looking at Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:01:54.122953Z",
     "start_time": "2021-04-05T21:01:53.710296Z"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import webbrowser\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "\n",
    "import config\n",
    "\n",
    "# auth.set_access_token(access_token, access_token_secret)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:19:13.987951Z",
     "start_time": "2021-04-02T15:19:13.985662Z"
    }
   },
   "outputs": [],
   "source": [
    "callback_uri = 'oob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:19:14.528430Z",
     "start_time": "2021-04-02T15:19:14.525488Z"
    }
   },
   "outputs": [],
   "source": [
    "auth = OAuthHandler(config.consumer_key, config.consumer_secret, callback_uri)\n",
    "auth.set_access_token(config.access_token, config.access_token_secret)\n",
    "# redirect_url = auth.get_authorization_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:50:25.672839Z",
     "start_time": "2021-04-01T20:50:25.529912Z"
    }
   },
   "outputs": [],
   "source": [
    "# webbrowser.open(redirect_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:50:25.678742Z",
     "start_time": "2021-04-01T20:50:24.989Z"
    }
   },
   "outputs": [],
   "source": [
    "# user_pin_input = input(\"What's the pin value? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:51:00.810502Z",
     "start_time": "2021-04-01T20:51:00.803291Z"
    }
   },
   "outputs": [],
   "source": [
    "# auth.get_access_token(user_pin_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:19:17.778293Z",
     "start_time": "2021-04-02T15:19:17.776032Z"
    }
   },
   "outputs": [],
   "source": [
    "api = API(auth, wait_on_rate_limit= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:19:18.852527Z",
     "start_time": "2021-04-02T15:19:18.428962Z"
    }
   },
   "outputs": [],
   "source": [
    "me = api.me()\n",
    "print(me.screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get text from your timeline into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:51:18.391879Z",
     "start_time": "2021-04-01T20:51:18.206486Z"
    }
   },
   "outputs": [],
   "source": [
    "my_timeline = api.home_timeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:51:19.232885Z",
     "start_time": "2021-04-01T20:51:19.230231Z"
    }
   },
   "outputs": [],
   "source": [
    "print(my_timeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:51:26.705357Z",
     "start_time": "2021-04-01T20:51:26.700824Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = set()\n",
    "allowed_types = [str, int]\n",
    "tweet_data = []\n",
    "\n",
    "for status in my_timeline:\n",
    "#     print(status.text)\n",
    "#     print (type(vas(status)))\n",
    "    status_dict = dict(vars(status))\n",
    "    keys = status_dict.keys()\n",
    "    single_tweet_data = {}\n",
    "    for k in keys:\n",
    "        try:\n",
    "            v_type = type(status_dict['k'])\n",
    "        except:\n",
    "            v_type = None\n",
    "            \n",
    "        if v_type != None:\n",
    "            if v_type in allowed_types:\n",
    "                single_tweet_data[k] = status_dict[k]\n",
    "                columns.add(k)\n",
    "    tweets_data.append(single_tweet_data )\n",
    "        \n",
    "header_cols = list(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:51:27.884925Z",
     "start_time": "2021-04-01T20:51:27.870516Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweet_data, columns= header_cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:51:38.636118Z",
     "start_time": "2021-04-01T20:51:38.052121Z"
    }
   },
   "outputs": [],
   "source": [
    "#. get_status?\n",
    "\n",
    "user = api.get_user('code')\n",
    "user_timeline = user.timeline()\n",
    "df2 = extract_timeline_as_df(user_timeline)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:51:35.188373Z",
     "start_time": "2021-04-01T20:51:35.183458Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_timeline_as_df(timeline_list):\n",
    "    columns = set()\n",
    "    allowed_types = [str, int]\n",
    "    tweets_data = []\n",
    "    for status in timeline_list:\n",
    "        status_dict = dict(vars(status))\n",
    "        keys = status_dict.keys()\n",
    "        single_tweet_data = {\"user\": status.user.screen_name, \"author\": status.author.screen_name}\n",
    "        for k in keys:\n",
    "            try:\n",
    "                v_type = type(status_dict[k])\n",
    "            except:\n",
    "                v_type = None\n",
    "            if v_type != None:\n",
    "                if v_type in allowed_types:\n",
    "                    single_tweet_data[k] = status_dict[k]\n",
    "                    columns.add(k)\n",
    "        tweets_data.append(single_tweet_data)\n",
    "\n",
    "\n",
    "    header_cols = list(columns)\n",
    "    header_cols.append('author')\n",
    "    df = pd.DataFrame(tweets_data, columns=header_cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:50:25.689440Z",
     "start_time": "2021-04-01T20:50:25.003Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = extract_timeline_as_df(user_timeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:50:25.690507Z",
     "start_time": "2021-04-01T20:50:25.005Z"
    }
   },
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:50:25.691497Z",
     "start_time": "2021-04-01T20:50:25.006Z"
    }
   },
   "outputs": [],
   "source": [
    "status_obj = api.get_status('1247966976951058433')\n",
    "status_obj.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:50:25.692441Z",
     "start_time": "2021-04-01T20:50:25.007Z"
    }
   },
   "outputs": [],
   "source": [
    "print(status_obj.geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:50:25.693296Z",
     "start_time": "2021-04-01T20:50:25.010Z"
    }
   },
   "outputs": [],
   "source": [
    "len(api.home_timeline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:50:25.694069Z",
     "start_time": "2021-04-01T20:50:25.011Z"
    }
   },
   "outputs": [],
   "source": [
    "other_user = 'therock'\n",
    "for i, status in enumerate(tweepy.Cursor(api.user_timeline, screen_name = other_user).items(20)):\n",
    "    print (i, status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:50:25.694922Z",
     "start_time": "2021-04-01T20:50:25.012Z"
    }
   },
   "outputs": [],
   "source": [
    "api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:50:25.695986Z",
     "start_time": "2021-04-01T20:50:25.013Z"
    }
   },
   "outputs": [],
   "source": [
    "# query = '#vaccine'\n",
    "# api.search(q = query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:50:25.697069Z",
     "start_time": "2021-04-01T20:50:25.015Z"
    }
   },
   "outputs": [],
   "source": [
    "query = '#vaccine covid'\n",
    "for i, status in enumerate(tweepy.Cursor(api.search, q= query).items(50)):\n",
    "    print (i, status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:51:53.837358Z",
     "start_time": "2021-04-01T20:51:53.834570Z"
    }
   },
   "outputs": [],
   "source": [
    "cursor = tweepy.Cursor(api.user_timeline, id= 'code', tweet_mode = 'extended').items(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:51:55.283304Z",
     "start_time": "2021-04-01T20:51:54.903967Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in cursor:\n",
    "    print(dir(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:52:15.101239Z",
     "start_time": "2021-04-01T20:52:15.098167Z"
    }
   },
   "outputs": [],
   "source": [
    "cursor = tweepy.Cursor(api.user_timeline, id= 'code', tweet_mode = 'extended').items(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:52:47.992726Z",
     "start_time": "2021-04-01T20:52:47.610368Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in cursor:\n",
    "    print(i.full_text, i.lang, i.geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:55:05.289395Z",
     "start_time": "2021-04-01T20:55:05.286422Z"
    }
   },
   "outputs": [],
   "source": [
    "cursor = tweepy.Cursor(api.search, q= 'vaccine', tweet_mode = 'extended').items(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:55:06.755655Z",
     "start_time": "2021-04-01T20:55:06.357473Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in cursor:\n",
    "    print(i.full_text, i.lang, i.geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T21:23:45.415267Z",
     "start_time": "2021-04-01T21:23:45.080720Z"
    }
   },
   "outputs": [],
   "source": [
    "number_of_tweets = 50\n",
    "tweets = []\n",
    "language = []\n",
    "time = []\n",
    "\n",
    "for i in tweepy.Cursor(api.search, q= 'vaccine covid -RT',  tweet_mode = 'extended', include_rts= False).items(number_of_tweets):\n",
    "    tweets.append(i.full_text)\n",
    "    language.append(i.lang)\n",
    "    time.append(i.created_at)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T21:23:46.465814Z",
     "start_time": "2021-04-01T21:23:46.462414Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'time': time, 'language': language, 'tweets': tweets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T21:23:47.484663Z",
     "start_time": "2021-04-01T21:23:47.476774Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T21:04:43.503842Z",
     "start_time": "2021-04-01T21:04:43.499893Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[~df.tweets.str.contains('RT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T21:05:12.977087Z",
     "start_time": "2021-04-01T21:05:12.974432Z"
    }
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop= True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T21:39:37.721239Z",
     "start_time": "2021-04-01T21:39:37.716993Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('tweets.csv', mode= 'a', index= False, header= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T21:34:34.782648Z",
     "start_time": "2021-04-01T21:34:34.773133Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('tweets.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T21:37:43.669307Z",
     "start_time": "2021-04-01T21:37:43.303352Z"
    }
   },
   "outputs": [],
   "source": [
    "number_of_tweets = 5\n",
    "tweets = []\n",
    "language = []\n",
    "time = []\n",
    "\n",
    "for i in tweepy.Cursor(api.search, q= 'vaccine covid -RT',  tweet_mode = 'extended', include_rts= False).items(number_of_tweets):\n",
    "    tweets.append(i.full_text)\n",
    "    language.append(i.lang)\n",
    "    time.append(i.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T21:37:56.057583Z",
     "start_time": "2021-04-01T21:37:56.049306Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'time': time, 'language': language, 'tweets': tweets})\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T21:38:30.567016Z",
     "start_time": "2021-04-01T21:38:30.563062Z"
    }
   },
   "outputs": [],
   "source": [
    "df3.to_csv('tweets.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search 30 day tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to turn results into DataFrame\n",
    "def extract_text_as_df(text_list):\n",
    "    columns = set()\n",
    "    allowed_types = [str, int]\n",
    "    tweets_data = []\n",
    "    for status in text_list:\n",
    "        status_dict = dict(vars(status))\n",
    "        keys = status_dict.keys()\n",
    "        single_tweet_data = {\"author\": status.author.screen_name}\n",
    "        for k in keys:\n",
    "            try:\n",
    "                v_type = type(status_dict[k])\n",
    "            except:\n",
    "                v_type = None\n",
    "            if v_type != None:\n",
    "                if v_type in allowed_types:\n",
    "                    single_tweet_data[k] = status_dict[k]\n",
    "                    columns.add(k)\n",
    "        tweets_data.append(single_tweet_data)\n",
    "\n",
    "\n",
    "    header_cols = list(columns)\n",
    "    header_cols.append('author')\n",
    "    df = pd.DataFrame(tweets_data, columns=header_cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T23:19:51.785007Z",
     "start_time": "2021-04-01T23:19:51.438306Z"
    }
   },
   "outputs": [],
   "source": [
    "query = 'vaccine covid -RT'\n",
    "results_30 = api.search_30_day('datacollection', query, maxResults= 10)\n",
    "# results_30\n",
    "old_results_df = extract_text_as_df(results_30)\n",
    "old_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T22:13:23.552431Z",
     "start_time": "2021-04-01T22:13:18.017332Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:20:00.461155Z",
     "start_time": "2021-04-02T15:20:00.426488Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:20:06.002568Z",
     "start_time": "2021-04-02T15:20:05.994391Z"
    }
   },
   "outputs": [],
   "source": [
    "lang_df = df.sort_values('language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:20:08.019351Z",
     "start_time": "2021-04-02T15:20:07.828114Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:20:08.948094Z",
     "start_time": "2021-04-02T15:20:08.657876Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (12, 8))\n",
    "lang_df.language.value_counts(normalize=True).plot(kind= 'barh')\n",
    "plt.title('Proportion of Languages Collected')\n",
    "plt.xlim(-.01, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:20:13.122991Z",
     "start_time": "2021-04-02T15:20:13.115004Z"
    }
   },
   "outputs": [],
   "source": [
    "lang_df.language.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:22:49.050148Z",
     "start_time": "2021-04-02T15:22:48.692536Z"
    }
   },
   "outputs": [],
   "source": [
    "# what are these languages? get table from url\n",
    "URL = 'https://developer.twitter.com/en/docs/twitter-for-websites/supported-languages'\n",
    "tables = pd.read_html(URL)\n",
    "print(\"There are : \",len(tables),\" tables\")\n",
    "print(tables[0])\n",
    "\n",
    "lang_table = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:22:53.363407Z",
     "start_time": "2021-04-02T15:22:53.350527Z"
    }
   },
   "outputs": [],
   "source": [
    "# lang_table\n",
    "header_row = 0\n",
    "lang_table.columns = lang_table.iloc[header_row]\n",
    "lang_table = lang_table.drop(header_row)\n",
    "lang_table.reset_index(drop= True, inplace= True)\n",
    "lang_table.replace('English (default)', 'English', inplace= True)\n",
    "lang_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:25:42.447826Z",
     "start_time": "2021-04-02T15:25:42.443338Z"
    }
   },
   "outputs": [],
   "source": [
    "lang_table.to_csv('data/languages.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:46:33.982159Z",
     "start_time": "2021-04-02T15:46:33.972596Z"
    }
   },
   "outputs": [],
   "source": [
    "duped = df[df.duplicated(subset= 'tweets')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:49:49.571048Z",
     "start_time": "2021-04-02T15:49:49.568338Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:49:50.613245Z",
     "start_time": "2021-04-02T15:49:50.604534Z"
    }
   },
   "outputs": [],
   "source": [
    "duped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:53:38.426807Z",
     "start_time": "2021-04-02T15:53:38.417025Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df.drop_duplicates(subset= 'tweets', keep= 'first', inplace= True)\n",
    "df.reset_index(drop= True, inplace= True)\n",
    "df.tweets.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:54:52.907824Z",
     "start_time": "2021-04-02T15:54:52.902869Z"
    }
   },
   "outputs": [],
   "source": [
    "df.tweets.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:01:18.156092Z",
     "start_time": "2021-04-02T16:01:18.145465Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df['language'] == 'und']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:03:23.631400Z",
     "start_time": "2021-04-02T16:03:23.626943Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['language'] != 'und']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:03:28.507886Z",
     "start_time": "2021-04-02T16:03:28.497926Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:20:47.381173Z",
     "start_time": "2021-04-02T16:20:47.377561Z"
    }
   },
   "outputs": [],
   "source": [
    "lang_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:22:12.558201Z",
     "start_time": "2021-04-02T16:22:12.548919Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:40:29.678460Z",
     "start_time": "2021-04-02T16:40:29.661836Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = df.merge(lang_table, how= 'inner', left_on= 'language', right_on= 'Language code')\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:40:31.892645Z",
     "start_time": "2021-04-02T16:40:31.880114Z"
    }
   },
   "outputs": [],
   "source": [
    "df2.drop(columns = ['language', 'Language code'], inplace= True)\n",
    "df2.rename(columns={'Name': 'language'}, inplace= True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:40:37.540734Z",
     "start_time": "2021-04-02T16:40:37.537360Z"
    }
   },
   "outputs": [],
   "source": [
    "df2.rename(columns={'Name': 'language'}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:41:38.226537Z",
     "start_time": "2021-04-02T16:41:38.219601Z"
    }
   },
   "outputs": [],
   "source": [
    "df2['time'] = pd.to_datetime(df2['time']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:41:39.602341Z",
     "start_time": "2021-04-02T16:41:39.592114Z"
    }
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:35:12.210595Z",
     "start_time": "2021-04-02T16:35:12.199871Z"
    }
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:44:37.443221Z",
     "start_time": "2021-04-02T16:44:37.440479Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:51:39.872983Z",
     "start_time": "2021-04-02T16:51:39.640312Z"
    }
   },
   "outputs": [],
   "source": [
    "# countplot that excludes English to see detail\n",
    "plt.figure(figsize= (10, 8))\n",
    "sns.countplot(y= 'language', data= df2, palette='autumn',\n",
    "              order= df2.language.value_counts().iloc[1:].index)\n",
    "plt.title('Count of Tweets, Excluding English', fontsize= 14)\n",
    "plt.xlabel('# of Tweets')\n",
    "plt.ylabel('Language')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls? https: http: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use textblob?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T18:26:52.737072Z",
     "start_time": "2021-04-04T18:26:52.681025Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/tweets_translated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T18:26:55.152195Z",
     "start_time": "2021-04-04T18:26:55.148236Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_df = df2[['time', 'tweets']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T20:35:06.996434Z",
     "start_time": "2021-04-04T20:35:06.993943Z"
    }
   },
   "outputs": [],
   "source": [
    "trial_df = sentiment_df[20:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T20:35:13.144620Z",
     "start_time": "2021-04-04T20:35:13.137293Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "trial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:13:50.242332Z",
     "start_time": "2021-04-05T15:13:50.240081Z"
    }
   },
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from google.cloud import language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T18:27:44.271655Z",
     "start_time": "2021-04-04T18:27:44.264852Z"
    }
   },
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file('../GCP_service_account.json')\n",
    "# translate_client = translate.Client(credentials= credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T18:34:23.593226Z",
     "start_time": "2021-04-04T18:34:23.590910Z"
    }
   },
   "outputs": [],
   "source": [
    "# client = language.LanguageServiceClient()\n",
    "nlp_client = language.Client(credentials = credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:58:32.578127Z",
     "start_time": "2021-04-05T16:58:32.575055Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "magnitudes = []\n",
    "\n",
    "def getGoogleSentiments(txt):\n",
    "    document = nlp_client.document_from_text(txt)\n",
    "    \n",
    "    sent_analysis = document.analyze_sentiment()\n",
    "    sentiment = sent_analysis.sentiment\n",
    "    \n",
    "    score = sentiment.score\n",
    "    scores.append(score)\n",
    "    magnitude = sentiment.magnitude\n",
    "    magnitudes.append(magnitude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add time delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T17:13:01.520241Z",
     "start_time": "2021-04-05T17:13:01.517176Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "magnitudes = []\n",
    "\n",
    "def getGoogleSentiments(txt):\n",
    "    document = nlp_client.document_from_text(txt)\n",
    "\n",
    "    time.sleep(5)\n",
    "    print('sleeping 1')\n",
    "    sent_analysis = document.analyze_sentiment()\n",
    "    time.sleep(5)\n",
    "    print('sleeping 2')\n",
    "    sentiment = sent_analysis.sentiment\n",
    "\n",
    "    score = sentiment.score\n",
    "    scores.append(score)\n",
    "    magnitude = sentiment.magnitude\n",
    "    magnitudes.append(magnitude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T17:14:15.902401Z",
     "start_time": "2021-04-05T17:13:03.195336Z"
    }
   },
   "outputs": [],
   "source": [
    "trying = trial_df.tweets.apply(getGoogleSentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_client.document_from_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T19:09:07.660488Z",
     "start_time": "2021-04-04T19:09:07.657627Z"
    }
   },
   "outputs": [],
   "source": [
    "def language_analysis(text):\n",
    "    document = nlp_client.document_from_text(text)\n",
    "    \n",
    "    sent_analysis = document.analyze_sentiment()\n",
    "#     print(dir(sent_analysis))\n",
    "    sentiment = sent_analysis.sentiment\n",
    "    \n",
    "    ent_analysis = document.analyze_entities()\n",
    "    entities = ent_analysis.entities\n",
    "    \n",
    "#     synt_analysis = document.synt_analysis()\n",
    "#     syntax = synt_analysis.syntax\n",
    "    \n",
    "    return sentiment, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# translate tweets into English\n",
    "for i in trial4_df.tweets.loc[:]:\n",
    "    \n",
    "    document = nlp_client.document_from_text(i)\n",
    "    sent_analysis = document.analyze_sentiment()\n",
    "    sentiment = sent_analysis.sentiment\n",
    "    \n",
    "    score = sentiment.score\n",
    "    magnitude = sentiment.magnitude\n",
    "    \n",
    "    result = translate_client.translate(i, target_language= 'en')\n",
    "    results[i] = result['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T19:09:10.773019Z",
     "start_time": "2021-04-04T19:09:10.518403Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example_text = 'Is it not obvious that Python is the best programming language of them all?'\n",
    "# example_text = 'Concerning the use of aborted baby tissue in the J&amp;J Covid vaccine:\\n\\n“The J&amp;J vaccine has sparked debate among some religious communities because it was developed from stem cells obtained during two abortions decades ago.”\\n\\nEvil is still evil, even if it was decades ago.'\n",
    "example_text = '@Kit_Yates_Maths Tests\\n1. Vax deployment ✅✅\\n2. Vax reduces hospitalisation ✅✅\\n3. Infection rates unsubstantiated pressure on NHS ✅\\n4. Variants of concern ☑️\\n\\nSo not measured but very conservative \\nhttps://t.co/iVMKF2pPsF https://t.co/ta4t21HH9s'\n",
    "\n",
    "sentiment, entities = language_analysis(example_text)\n",
    "print(sentiment.score, sentiment.magnitude)\n",
    "print()\n",
    "for e in entities:\n",
    "    print (e.name, e.entity_type, e.metadata, e.salience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T14:06:57.471214Z",
     "start_time": "2021-04-05T14:06:57.461312Z"
    }
   },
   "outputs": [],
   "source": [
    "trial3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:00:52.106635Z",
     "start_time": "2021-04-05T15:00:52.103401Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "magnitudes = []\n",
    "\n",
    "def getGoogleSentiments(txt):\n",
    "    document = nlp_client.document_from_text(txt)\n",
    "    \n",
    "    sent_analysis = document.analyze_sentiment()\n",
    "    sentiment = sent_analysis.sentiment\n",
    "    \n",
    "    score = sentiment.score\n",
    "    scores.append(score)\n",
    "    magnitude = sentiment.magnitude\n",
    "    magnitudes.append(magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:08:18.352931Z",
     "start_time": "2021-04-05T15:08:18.350525Z"
    }
   },
   "outputs": [],
   "source": [
    "trial4_df = sentiment_df[50:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:00:59.770781Z",
     "start_time": "2021-04-05T15:00:58.527538Z"
    }
   },
   "outputs": [],
   "source": [
    "ans4 = trial4_df.tweets.apply(getGoogleSentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:02:29.488587Z",
     "start_time": "2021-04-05T15:02:29.485240Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial4_df['Google Score'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:08:23.136373Z",
     "start_time": "2021-04-05T15:08:23.129041Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:09:17.248067Z",
     "start_time": "2021-04-05T15:09:16.039846Z"
    }
   },
   "outputs": [],
   "source": [
    "trial4_df.tweets.apply(lambda x: getGoogleSentiments(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:09:18.487359Z",
     "start_time": "2021-04-05T15:09:18.481171Z"
    }
   },
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T14:47:14.331523Z",
     "start_time": "2021-04-05T14:47:14.327918Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(trial2_df)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:09:10.769059Z",
     "start_time": "2021-04-05T15:09:10.763633Z"
    }
   },
   "outputs": [],
   "source": [
    "#try this one\n",
    "res_df = pd.DataFrame(columns = ['GoogSc', 'GoogMag'])\n",
    "df_len = len(trial3_df)\n",
    "scores = []\n",
    "magnitudes = []\n",
    "\n",
    "def getGoogleSentiments(txt):\n",
    "    document = nlp_client.document_from_text(txt)\n",
    "    \n",
    "    sent_analysis = document.analyze_sentiment()\n",
    "    sentiment = sent_analysis.sentiment\n",
    "    \n",
    "    score = sentiment.score\n",
    "    scores.append(score)\n",
    "    magnitude = sentiment.magnitude\n",
    "    magnitudes.append(magnitude)\n",
    "    \n",
    "    res_df = [[scores, magnitudes]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:21:58.068743Z",
     "start_time": "2021-04-05T15:21:56.636429Z"
    }
   },
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(columns = ['GoogSc', 'GoogMag'])\n",
    "\n",
    "# translate tweets into English\n",
    "for i in trial4_df.tweets.loc[:]:\n",
    "    \n",
    "    document = nlp_client.document_from_text(i)\n",
    "    sent_analysis = document.analyze_sentiment()\n",
    "    sentiment = sent_analysis.sentiment\n",
    "    \n",
    "    score = sentiment.score\n",
    "    magnitude = sentiment.magnitude\n",
    "    \n",
    "#     result = translate_client.translate(i, target_language= 'en')\n",
    "    res_df[i] = [score, magnitude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:22:09.353593Z",
     "start_time": "2021-04-05T15:22:09.337603Z"
    }
   },
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T14:57:49.228753Z",
     "start_time": "2021-04-05T14:57:44.905274Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial3_df.tweets.apply(getGoogleSentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T14:58:41.655691Z",
     "start_time": "2021-04-05T14:58:41.652003Z"
    }
   },
   "outputs": [],
   "source": [
    "df_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T14:57:53.514403Z",
     "start_time": "2021-04-05T14:57:53.508576Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial3_df.merge(res_df, left_on= 'tweet', right_on= res_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T13:34:27.621151Z",
     "start_time": "2021-04-05T13:34:27.618112Z"
    }
   },
   "outputs": [],
   "source": [
    "def getGoogleSentiments(txt):\n",
    "    document = nlp_client.document_from_text(txt)\n",
    "    \n",
    "    sent_analysis = document.analyze_sentiment()\n",
    "    sentiment = sent_analysis.sentiment\n",
    "    \n",
    "    score = sentiment.score\n",
    "    magnitude = sentiment.magnitude\n",
    "    \n",
    "    return np.array((score, magnitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T14:07:18.909599Z",
     "start_time": "2021-04-05T14:07:15.199976Z"
    }
   },
   "outputs": [],
   "source": [
    "answer = trial3_df['tweets'].apply(getGoogleSentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T14:34:02.615421Z",
     "start_time": "2021-04-05T14:34:02.611812Z"
    }
   },
   "outputs": [],
   "source": [
    "trial3_df.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T14:20:22.952277Z",
     "start_time": "2021-04-05T14:20:22.949492Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "mags = []\n",
    "\n",
    "for s, m in answer:\n",
    "    scores.append(s)\n",
    "    mags.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T14:24:09.675275Z",
     "start_time": "2021-04-05T14:24:09.671388Z"
    }
   },
   "outputs": [],
   "source": [
    "trial3_df['GooglesScores'].loc[:] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T14:24:20.710422Z",
     "start_time": "2021-04-05T14:24:20.695382Z"
    }
   },
   "outputs": [],
   "source": [
    "trial3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T14:18:19.009785Z",
     "start_time": "2021-04-05T14:18:18.998807Z"
    }
   },
   "outputs": [],
   "source": [
    "trial3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T13:38:09.478759Z",
     "start_time": "2021-04-05T13:38:09.462622Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial2_df[['Google Score', 'Google Magnitude']] = trial2_df['tweets'].apply(getGoogleSentiments, axis= 1, result_type= 'expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T13:56:27.164757Z",
     "start_time": "2021-04-05T13:56:27.155455Z"
    }
   },
   "outputs": [],
   "source": [
    "trial2_df[['Google Score','Google Magnitude']] = trial2_df['tweets'].apply(lambda x: (x[0], x[1]) for x in getGoogleSentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T13:29:50.042451Z",
     "start_time": "2021-04-05T13:29:50.029740Z"
    }
   },
   "outputs": [],
   "source": [
    "trial2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T19:02:08.536073Z",
     "start_time": "2021-04-04T19:02:08.533333Z"
    }
   },
   "source": [
    "**score** of the sentiment ranges between -1.0 (negative) and 1.0 (positive) and corresponds to the overall emotional leaning of the text.\n",
    "\n",
    "**magnitude** indicates the overall strength of emotion (both positive and negative) within the given text, between 0.0 and +inf. Unlike score, magnitude is not normalized; each expression of emotion within the text (both positive and negative) contributes to the text's magnitude (so longer text blocks may have greater magnitudes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T20:36:38.270610Z",
     "start_time": "2021-04-04T20:36:38.267286Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in trial_df.tweets:\n",
    "#     print(i)\n",
    "    \n",
    "result = {}\n",
    "\n",
    "for text in example_text:\n",
    "    sentiment, entities = language_analysis(text)\n",
    "    result['score'] = sentiment.score\n",
    "    result['magnitude'] = sentiment.magnitude\n",
    "    \n",
    "    for e in entities:\n",
    "        result['entity name'] = e.name\n",
    "        result['entity salience'] = e.salience\n",
    "        \n",
    "    google_nlp_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing it the hard way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T18:32:14.046778Z",
     "start_time": "2021-04-05T18:32:14.018208Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import re \n",
    "from textblob import TextBlob\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokens = []\n",
    "for sent in compare_list:\n",
    "    print(tweet_tokenizer.tokenize(sent))\n",
    "    tweet_tokens.append(tweet_tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T22:57:02.965751Z",
     "start_time": "2021-04-04T22:57:02.962960Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|(amp)\", \n",
    "                           \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:08:48.425335Z",
     "start_time": "2021-04-04T21:08:48.422416Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_tweet_sentiment(tweet):\n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "    if analysis.sentiment.polarity < 0:\n",
    "        return 'negative'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T22:56:54.533653Z",
     "start_time": "2021-04-04T22:56:54.531151Z"
    }
   },
   "outputs": [],
   "source": [
    "trial3_df = sentiment_df[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T22:57:06.269986Z",
     "start_time": "2021-04-04T22:57:06.265473Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_tweets = []\n",
    "\n",
    "for i in trial3_df.tweets:\n",
    "    cleaned = clean_tweet(i)\n",
    "    cleaned_tweets.append(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T22:57:08.895170Z",
     "start_time": "2021-04-04T22:57:08.891005Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:08:52.084914Z",
     "start_time": "2021-04-04T21:08:52.077612Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_nums = []\n",
    "\n",
    "for i in cleaned_tweets:\n",
    "    sent_nums = get_tweet_sentiment(i)\n",
    "    sentiment_nums.append(sent_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:08:52.768876Z",
     "start_time": "2021-04-04T21:08:52.765256Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:10:04.608122Z",
     "start_time": "2021-04-04T21:10:04.604701Z"
    }
   },
   "outputs": [],
   "source": [
    "trial_df['TBlob sent'] = sentiment_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:10:16.043326Z",
     "start_time": "2021-04-04T21:10:16.034459Z"
    }
   },
   "outputs": [],
   "source": [
    "trial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:15:19.219747Z",
     "start_time": "2021-04-04T21:15:19.210238Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_nummms = []\n",
    "\n",
    "for i in trial_df.tweets:\n",
    "    cleaned = clean_tweet(i)\n",
    "    sent_nums = get_tweet_sentiment(cleaned)\n",
    "    sentiment_nummms.append(sent_nums)\n",
    "    \n",
    "sentiment_nummms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:25:50.407991Z",
     "start_time": "2021-04-04T21:25:50.405518Z"
    }
   },
   "outputs": [],
   "source": [
    "trial2_df = sentiment_df[35:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:26:49.314796Z",
     "start_time": "2021-04-04T21:26:49.310407Z"
    }
   },
   "outputs": [],
   "source": [
    "trial2_df['tweets'] = trial2_df['tweets'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:27:22.286891Z",
     "start_time": "2021-04-04T21:27:22.284036Z"
    }
   },
   "outputs": [],
   "source": [
    "def getTextSubjectivity(txt):\n",
    "    return TextBlob(txt).sentiment.subjectivity\n",
    "\n",
    "def getTextPolarity(txt):\n",
    "    return TextBlob(txt).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:28:25.506307Z",
     "start_time": "2021-04-04T21:28:25.485744Z"
    }
   },
   "outputs": [],
   "source": [
    "trial2_df['Subjectivity'] = trial2_df['tweets'].apply(getTextSubjectivity)\n",
    "trial2_df['Polarity'] = trial2_df['tweets'].apply(getTextPolarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:30:07.592568Z",
     "start_time": "2021-04-04T21:30:07.571711Z"
    }
   },
   "outputs": [],
   "source": [
    "trial2_df['TB Sentiment'] = trial2_df['tweets'].apply(get_tweet_sentiment)\n",
    "trial2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing for google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:35:45.684973Z",
     "start_time": "2021-04-05T16:35:45.682482Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:49:06.274124Z",
     "start_time": "2021-04-05T16:49:06.271427Z"
    }
   },
   "outputs": [],
   "source": [
    "listl = [1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:49:43.090566Z",
     "start_time": "2021-04-05T16:49:25.067074Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(listl)):\n",
    "    print(i)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:54:21.990233Z",
     "start_time": "2021-04-05T16:54:21.987721Z"
    }
   },
   "outputs": [],
   "source": [
    "def time_delay(i):\n",
    "    print('here we go')\n",
    "\n",
    "    print('here we are')\n",
    "\n",
    "    print('there it went')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:54:49.757575Z",
     "start_time": "2021-04-05T16:54:31.739841Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(listl)):\n",
    "    time.sleep(3)\n",
    "    time_delay(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T20:38:48.771040Z",
     "start_time": "2021-04-05T20:38:48.711492Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_df = pd.read_csv('data/tweets_translated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:08:46.939149Z",
     "start_time": "2021-04-05T21:08:46.935392Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions to clean tweets, get subjectivity and polarity scores and sentiment\n",
    "def cleanTweet(txt):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|(amp)\", \n",
    "                           \" \", str(txt)).split())\n",
    "\n",
    "def getTextSubjectivity(txt):\n",
    "    '''Subjective sentences generally refer to personal opinion, emotion \n",
    "    or judgment whereas objective refers to factual information. \n",
    "    Subjectivity is also a float which lies in the range of [0,1].'''\n",
    "    return TextBlob(txt).sentiment.subjectivity\n",
    "\n",
    "def getTextPolarity(txt):\n",
    "    '''Polarity is float which lies in the range of [-1,1] \n",
    "    where 1 means positive statement and -1 means a negative statement.'''\n",
    "    return TextBlob(txt).sentiment.polarity\n",
    "\n",
    "# function to determine if Tweet was positive, negative, or neutral\n",
    "def getTextAnalysis(p):\n",
    "    if p < 0:\n",
    "        return 'Negative'\n",
    "    elif p == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T20:40:16.114866Z",
     "start_time": "2021-04-05T20:40:08.463767Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply functions to tweets\n",
    "sentiment_df['tweets'] = sentiment_df['tweets'].apply(cleanTweet)\n",
    "sentiment_df['TB Subjectivity'] = sentiment_df['tweets'].apply(getTextSubjectivity)\n",
    "sentiment_df['TB Polarity'] = sentiment_df['tweets'].apply(getTextPolarity)\n",
    "sentiment_df['TB Sentiment'] = sentiment_df['TB Polarity'].apply(getTextAnalysis)\n",
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T20:41:30.629596Z",
     "start_time": "2021-04-05T20:41:30.509102Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_df.to_csv('data/text_sentiment_TextBlob.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:02:00.765832Z",
     "start_time": "2021-04-05T21:01:59.511223Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cassienutter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize, TweetTokenizer, FreqDist\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re \n",
    "from textblob import TextBlob\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tokenize(tweet):\n",
    "    tknzr = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "    return tknzr.tokenize(tweet)\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:04:36.339030Z",
     "start_time": "2021-04-05T21:04:36.294620Z"
    }
   },
   "outputs": [],
   "source": [
    "dataf = pd.read_csv('data/text_sentiment_TextBlob.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:06:08.701162Z",
     "start_time": "2021-04-05T21:06:08.683947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>tweets</th>\n",
       "      <th>language</th>\n",
       "      <th>TB Subjectivity</th>\n",
       "      <th>TB Polarity</th>\n",
       "      <th>TB Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>Concerning the use of aborted baby tissue in t...</td>\n",
       "      <td>English</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>-0.475000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>imagine an animal getting the covid 19 vaccine...</td>\n",
       "      <td>English</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>Definition of a microstate Source of vaccine data</td>\n",
       "      <td>English</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>If I ve Had COVID 19 Do I Still Need Two Doses...</td>\n",
       "      <td>English</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>My epidural was delayed again somehow my covid...</td>\n",
       "      <td>English</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18570</th>\n",
       "      <td>2021-04-02</td>\n",
       "      <td>Umrah can be performed without vaccination in ...</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18571</th>\n",
       "      <td>2021-04-03</td>\n",
       "      <td>Nurse busy on mobile one person was vaccinated...</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18572</th>\n",
       "      <td>2021-04-03</td>\n",
       "      <td>While talking on the phone the nurse vaccinate...</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18573</th>\n",
       "      <td>2021-04-04</td>\n",
       "      <td>Covid 19 Vaccine Registration Easy way to regi...</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18574</th>\n",
       "      <td>2021-04-03</td>\n",
       "      <td>of course the number says something now I am m...</td>\n",
       "      <td>Norwegian</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18575 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             time                                             tweets  \\\n",
       "0      2021-04-01  Concerning the use of aborted baby tissue in t...   \n",
       "1      2021-04-01  imagine an animal getting the covid 19 vaccine...   \n",
       "2      2021-04-01  Definition of a microstate Source of vaccine data   \n",
       "3      2021-04-01  If I ve Had COVID 19 Do I Still Need Two Doses...   \n",
       "4      2021-04-01  My epidural was delayed again somehow my covid...   \n",
       "...           ...                                                ...   \n",
       "18570  2021-04-02  Umrah can be performed without vaccination in ...   \n",
       "18571  2021-04-03  Nurse busy on mobile one person was vaccinated...   \n",
       "18572  2021-04-03  While talking on the phone the nurse vaccinate...   \n",
       "18573  2021-04-04  Covid 19 Vaccine Registration Easy way to regi...   \n",
       "18574  2021-04-03  of course the number says something now I am m...   \n",
       "\n",
       "        language  TB Subjectivity  TB Polarity TB Sentiment  \n",
       "0        English         0.637500    -0.475000     Negative  \n",
       "1        English         0.100000     0.000000      Neutral  \n",
       "2        English         0.000000     0.000000      Neutral  \n",
       "3        English         0.000000     0.000000      Neutral  \n",
       "4        English         0.575000     0.033333     Positive  \n",
       "...          ...              ...          ...          ...  \n",
       "18570    Bengali         0.000000     0.000000      Neutral  \n",
       "18571    Bengali         0.300000     0.100000     Positive  \n",
       "18572    Bengali         0.000000     0.000000      Neutral  \n",
       "18573    Bengali         0.833333     0.433333     Positive  \n",
       "18574  Norwegian         0.575000     0.200000     Positive  \n",
       "\n",
       "[18575 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:08:53.335677Z",
     "start_time": "2021-04-05T21:08:52.999195Z"
    }
   },
   "outputs": [],
   "source": [
    "dataf['tweets'] = dataf['tweets'].apply(cleanTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T18:38:45.945969Z",
     "start_time": "2021-04-05T18:38:45.605175Z"
    }
   },
   "outputs": [],
   "source": [
    "data = dataf['tweets'].map(TweetTokenizer).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T19:17:17.082425Z",
     "start_time": "2021-04-05T19:17:17.068140Z"
    }
   },
   "outputs": [],
   "source": [
    "dataaa = dataf['tweets'].map(TweetTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T19:17:29.406795Z",
     "start_time": "2021-04-05T19:17:29.401980Z"
    }
   },
   "outputs": [],
   "source": [
    "dataaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:09:05.279375Z",
     "start_time": "2021-04-05T21:09:03.045573Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = dataf['tweets'].map(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:09:08.833939Z",
     "start_time": "2021-04-05T21:09:08.830336Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:09:10.457221Z",
     "start_time": "2021-04-05T21:09:10.451210Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [Concerning, the, use, of, aborted, baby, tiss...\n",
       "1        [imagine, an, animal, getting, the, covid, 19,...\n",
       "2        [Definition, of, a, microstate, Source, of, va...\n",
       "3        [If, I, ve, Had, COVID, 19, Do, I, Still, Need...\n",
       "4        [My, epidural, was, delayed, again, somehow, m...\n",
       "                               ...                        \n",
       "18570    [Umrah, can, be, performed, without, vaccinati...\n",
       "18571    [Nurse, busy, on, mobile, one, person, was, va...\n",
       "18572    [While, talking, on, the, phone, the, nurse, v...\n",
       "18573    [Covid, 19, Vaccine, Registration, Easy, way, ...\n",
       "18574    [of, course, the, number, says, something, now...\n",
       "Name: tweets, Length: 18575, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T19:12:54.937891Z",
     "start_time": "2021-04-05T19:12:54.932835Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for l in tokens[:3]:\n",
    "#     print([w.lower() for w in l])\n",
    "    \n",
    "    \n",
    "for l in tokens[:3]:\n",
    "    for w in l:\n",
    "        if w not in stopwords_list:\n",
    "            print(w.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T19:00:22.847737Z",
     "start_time": "2021-04-05T19:00:22.834955Z"
    }
   },
   "outputs": [],
   "source": [
    "stopped_data = [w.lower() for w in tokens[:] if w not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:09:31.037816Z",
     "start_time": "2021-04-05T21:09:30.122676Z"
    }
   },
   "outputs": [],
   "source": [
    "#Use this one\n",
    "stopped_data = [w.lower() for l in tokens for w in l if w not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:09:45.907400Z",
     "start_time": "2021-04-05T21:09:45.903945Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315172"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:09:58.242631Z",
     "start_time": "2021-04-05T21:09:58.227767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['concerning',\n",
       " 'use',\n",
       " 'aborted',\n",
       " 'baby',\n",
       " 'tissue',\n",
       " 'j',\n",
       " 'j',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'the',\n",
       " 'j',\n",
       " 'j',\n",
       " 'vaccine',\n",
       " 'sparked',\n",
       " 'debate',\n",
       " 'among',\n",
       " 'religious',\n",
       " 'communities',\n",
       " 'developed',\n",
       " 'stem',\n",
       " 'cells',\n",
       " 'obtained',\n",
       " 'two',\n",
       " 'abortions',\n",
       " 'decades',\n",
       " 'ago',\n",
       " 'evil',\n",
       " 'still',\n",
       " 'evil',\n",
       " 'even',\n",
       " 'decades',\n",
       " 'ago',\n",
       " 'imagine',\n",
       " 'animal',\n",
       " 'getting',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'b4',\n",
       " 'human',\n",
       " 'definition',\n",
       " 'microstate',\n",
       " 'source',\n",
       " 'vaccine',\n",
       " 'data',\n",
       " 'if',\n",
       " 'i',\n",
       " 'had',\n",
       " 'covid',\n",
       " '19',\n",
       " 'do',\n",
       " 'i',\n",
       " 'still',\n",
       " 'need',\n",
       " 'two',\n",
       " 'doses',\n",
       " 'vaccine',\n",
       " 'my',\n",
       " 'epidural',\n",
       " 'delayed',\n",
       " 'somehow',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'interact',\n",
       " 'well',\n",
       " 'epidural',\n",
       " 'sooooo',\n",
       " 'i',\n",
       " 'getting',\n",
       " 'birthday',\n",
       " 'yay',\n",
       " 'so',\n",
       " 'far',\n",
       " 'i',\n",
       " '1',\n",
       " '2',\n",
       " 'dr',\n",
       " 'appointments',\n",
       " 'week',\n",
       " 'entire',\n",
       " 'month',\n",
       " 'on',\n",
       " 'upside',\n",
       " 'given',\n",
       " 'proper',\n",
       " 'muscle',\n",
       " 'relaxers',\n",
       " 'time',\n",
       " 'blake',\n",
       " 'lively',\n",
       " 'hilariously',\n",
       " 'called',\n",
       " 'husband',\n",
       " 'ryan',\n",
       " 'reynolds',\n",
       " 'posted',\n",
       " 'got',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'shots',\n",
       " 'yates',\n",
       " 'maths',\n",
       " 'tests',\n",
       " '1',\n",
       " 'vax',\n",
       " 'deployment',\n",
       " '2',\n",
       " 'vax',\n",
       " 'reduces',\n",
       " 'hospitalisation',\n",
       " '3',\n",
       " 'infection',\n",
       " 'rates',\n",
       " 'unsubstantiated',\n",
       " 'pressure',\n",
       " 'nhs',\n",
       " '4',\n",
       " 'variants',\n",
       " 'concern',\n",
       " 'so',\n",
       " 'measured',\n",
       " 'conservative',\n",
       " 'here',\n",
       " 'forced',\n",
       " 'get',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'us',\n",
       " 'i',\n",
       " 'received',\n",
       " 'second',\n",
       " 'dose',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'today',\n",
       " 'i',\n",
       " 'grateful',\n",
       " 'hopeful',\n",
       " 'return',\n",
       " 'normalcy',\n",
       " 'horizon',\n",
       " 'i',\n",
       " 'want',\n",
       " 'thank',\n",
       " 'everyone',\n",
       " 'working',\n",
       " 'hard',\n",
       " 'get',\n",
       " 'folks',\n",
       " 'vaccinated',\n",
       " 'get',\n",
       " 'vaccine',\n",
       " 'getyourshot',\n",
       " 'vaccinated',\n",
       " 'ok',\n",
       " 'many',\n",
       " 'times',\n",
       " 'how',\n",
       " 'vaccine',\n",
       " 'clinics',\n",
       " 'areas',\n",
       " 'covid',\n",
       " 'cases',\n",
       " 'highest',\n",
       " 'or',\n",
       " 'important',\n",
       " 'paid',\n",
       " 'sick',\n",
       " 'leave',\n",
       " 'please',\n",
       " 'mr',\n",
       " 'matlow',\n",
       " 'ordinary',\n",
       " 'citizen',\n",
       " 'it',\n",
       " 'seems',\n",
       " 'unable',\n",
       " 'anything',\n",
       " 'i',\n",
       " 'pleading',\n",
       " 'something',\n",
       " 'ongoing',\n",
       " 'trial',\n",
       " 'shows',\n",
       " 'pfizer',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'remains',\n",
       " 'highly',\n",
       " 'effective',\n",
       " 'six',\n",
       " 'months',\n",
       " 'tx',\n",
       " 'j',\n",
       " 'children',\n",
       " 'ages',\n",
       " '1',\n",
       " '12',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'trial',\n",
       " 'completed',\n",
       " '3',\n",
       " 'boys',\n",
       " 'aged',\n",
       " '5',\n",
       " '9',\n",
       " 'waiting',\n",
       " 'rolled',\n",
       " 'sleeves',\n",
       " 'gettheshot',\n",
       " 'getvaccinated',\n",
       " 'covid19',\n",
       " 'over',\n",
       " '5m',\n",
       " 'canadians',\n",
       " 'given',\n",
       " 'least',\n",
       " 'one',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'dose',\n",
       " 'rollout',\n",
       " 'picks',\n",
       " 'steam',\n",
       " 'exercise',\n",
       " 'not',\n",
       " 'vaccine',\n",
       " 'i',\n",
       " 'students',\n",
       " 'young',\n",
       " 'healthy',\n",
       " 'got',\n",
       " 'covid',\n",
       " 'they',\n",
       " 'survived',\n",
       " 'ongoing',\n",
       " 'lung',\n",
       " 'problems',\n",
       " 'everyone',\n",
       " 'needs',\n",
       " 'vaccinate',\n",
       " 'spain',\n",
       " 'madrid',\n",
       " 'region',\n",
       " 'halts',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccinations',\n",
       " 'health',\n",
       " 'centers',\n",
       " 'four',\n",
       " 'days',\n",
       " 'medical',\n",
       " 'staff',\n",
       " 'rest',\n",
       " 'easter',\n",
       " 'holiday',\n",
       " 'despite',\n",
       " 'pleas',\n",
       " 'national',\n",
       " 'government',\n",
       " 'pause',\n",
       " 'fight',\n",
       " 'surging',\n",
       " 'infections',\n",
       " 'unc',\n",
       " 'home',\n",
       " 'particularly',\n",
       " 'hungry',\n",
       " 'lab',\n",
       " 'mice',\n",
       " 'starting',\n",
       " 'august',\n",
       " '2015',\n",
       " 'right',\n",
       " 'dr',\n",
       " 'baric',\n",
       " 'mastering',\n",
       " 'creation',\n",
       " 'chimeric',\n",
       " 'sars',\n",
       " 'like',\n",
       " 'coronaviruses',\n",
       " 'lab',\n",
       " 'kind',\n",
       " 'thing',\n",
       " 'might',\n",
       " 'want',\n",
       " 'template',\n",
       " 'live',\n",
       " 'attenuated',\n",
       " 'vaccine',\n",
       " 'vermont',\n",
       " 'different',\n",
       " 'formal',\n",
       " 'eligibility',\n",
       " 'rules',\n",
       " 'based',\n",
       " 'whether',\n",
       " 'bipoc',\n",
       " 'household',\n",
       " 'covid',\n",
       " 'cases',\n",
       " 'spiking',\n",
       " 'massachusetts',\n",
       " 'communities',\n",
       " 'high',\n",
       " 'risk',\n",
       " 'b',\n",
       " '1',\n",
       " '1',\n",
       " '7',\n",
       " 'widespread',\n",
       " 'massachusetts',\n",
       " 'circulating',\n",
       " 'within',\n",
       " 'communities',\n",
       " 'it',\n",
       " 'critical',\n",
       " 'everything',\n",
       " 'possible',\n",
       " 'keep',\n",
       " 'variants',\n",
       " 'concern',\n",
       " 'becoming',\n",
       " 'widespread',\n",
       " 'the',\n",
       " 'best',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'whatever',\n",
       " 'get',\n",
       " 'opposed',\n",
       " 'getting',\n",
       " 'one',\n",
       " 'why',\n",
       " 'compare',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccines',\n",
       " 'via',\n",
       " 'mishra21',\n",
       " 'y',\n",
       " 'better',\n",
       " 'two',\n",
       " 'doses',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'passing',\n",
       " 'around',\n",
       " 'i',\n",
       " 'buying',\n",
       " '3',\n",
       " 'things',\n",
       " 'know',\n",
       " 'hhs',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'ad',\n",
       " 'c',\n",
       " 'aign',\n",
       " 'leadership',\n",
       " 'group',\n",
       " 'we',\n",
       " 'got',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'appointments',\n",
       " 'open',\n",
       " 'twin',\n",
       " 'falls',\n",
       " 'boise',\n",
       " 'saturday',\n",
       " 'garda',\n",
       " 'unrest',\n",
       " 'vaccine',\n",
       " 'rollout',\n",
       " '100',\n",
       " 'receive',\n",
       " 'surplus',\n",
       " 'covid',\n",
       " 'jabs',\n",
       " 'over',\n",
       " '5m',\n",
       " 'canadians',\n",
       " 'given',\n",
       " 'least',\n",
       " 'one',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'dose',\n",
       " 'rollout',\n",
       " 'picks',\n",
       " 'steam',\n",
       " 'excellence',\n",
       " 'u',\n",
       " 'rehabilitate',\n",
       " 'patch',\n",
       " 'patch',\n",
       " 'oun',\n",
       " 'eba',\n",
       " 'loni',\n",
       " 'itan',\n",
       " 'ni',\n",
       " 'lola',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'circulating',\n",
       " 'ota',\n",
       " 'btw',\n",
       " 'there',\n",
       " 'two',\n",
       " 'cdc',\n",
       " 'pages',\n",
       " 'state',\n",
       " 'continue',\n",
       " 'wear',\n",
       " 'masks',\n",
       " 'go',\n",
       " 'away',\n",
       " 'id1ot',\n",
       " 'you',\n",
       " 'wheelhouse',\n",
       " 'vaccinated',\n",
       " 'heres',\n",
       " 'shouldnt',\n",
       " 'stop',\n",
       " 'wearing',\n",
       " 'face',\n",
       " 'mask',\n",
       " 'yet',\n",
       " 'my',\n",
       " 'mom',\n",
       " 'got',\n",
       " 'first',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'today',\n",
       " 'stop',\n",
       " 'flattering',\n",
       " 'modiji',\n",
       " 'work',\n",
       " 'covid',\n",
       " 'open',\n",
       " 'vaccine',\n",
       " 'all',\n",
       " 'asap',\n",
       " 'he',\n",
       " 'rly',\n",
       " 'said',\n",
       " 'get',\n",
       " 'covid',\n",
       " 'ever',\n",
       " 'get',\n",
       " 'getting',\n",
       " 'vaccine',\n",
       " 'means',\n",
       " 'get',\n",
       " 'vaccines',\n",
       " 'forever',\n",
       " 'dumb',\n",
       " 'i',\n",
       " 'even',\n",
       " 'arguing',\n",
       " 'back',\n",
       " 'i',\n",
       " 'like',\n",
       " 'ok',\n",
       " 'kept',\n",
       " 'going',\n",
       " 'like',\n",
       " 'shut',\n",
       " 'pfizer',\n",
       " 'fridge',\n",
       " 'ready',\n",
       " 'version',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'several',\n",
       " 'months',\n",
       " 'i',\n",
       " 'would',\n",
       " 'reconsider',\n",
       " 'position',\n",
       " 'i',\n",
       " 'as',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'myths',\n",
       " 'persist',\n",
       " 'usf',\n",
       " 'health',\n",
       " 'expert',\n",
       " 'shares',\n",
       " 'the',\n",
       " 'facts',\n",
       " 'boasting',\n",
       " 'success',\n",
       " 'time',\n",
       " 'like',\n",
       " 'shows',\n",
       " 'stunning',\n",
       " 'level',\n",
       " 'tone',\n",
       " 'deafness',\n",
       " 'it',\n",
       " 'hurts',\n",
       " 'see',\n",
       " 'province',\n",
       " 'going',\n",
       " 'celebrate',\n",
       " 'salt',\n",
       " 'fresh',\n",
       " 'wound',\n",
       " 'why',\n",
       " 'didnt',\n",
       " 'vax',\n",
       " 'arrive',\n",
       " '2',\n",
       " 'months',\n",
       " 'sooner',\n",
       " 'what',\n",
       " 'stopped',\n",
       " 'us',\n",
       " 'be',\n",
       " 'accountable',\n",
       " 'the',\n",
       " 'number',\n",
       " 'covid',\n",
       " '19',\n",
       " 'variant',\n",
       " 'cases',\n",
       " 'increasing',\n",
       " 'santaclaracounty',\n",
       " 'worrying',\n",
       " 'trend',\n",
       " 'comes',\n",
       " 'county',\n",
       " 'faces',\n",
       " 'shortage',\n",
       " 'vaccine',\n",
       " 'doses',\n",
       " 'seattle',\n",
       " 'area',\n",
       " '1',\n",
       " 'united',\n",
       " 'states',\n",
       " 'willingness',\n",
       " 'get',\n",
       " 'covid19',\n",
       " 'vaccine',\n",
       " 'vaccineswork',\n",
       " 'via',\n",
       " 'only',\n",
       " 'six',\n",
       " 'months',\n",
       " 'that',\n",
       " 'good',\n",
       " 'thing',\n",
       " 'pfizer',\n",
       " 'says',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'lasts',\n",
       " '6',\n",
       " 'months',\n",
       " 'protects',\n",
       " 'variants',\n",
       " 'via',\n",
       " 'i',\n",
       " 'anti',\n",
       " 'vaccine',\n",
       " 'this',\n",
       " 'sad',\n",
       " 'reality',\n",
       " 'covid',\n",
       " 'want',\n",
       " 'something',\n",
       " 'personally',\n",
       " 'get',\n",
       " 'landed',\n",
       " 'anti',\n",
       " 'vac',\n",
       " 'a',\n",
       " 'new',\n",
       " 'large',\n",
       " 'scale',\n",
       " 'vaccination',\n",
       " 'site',\n",
       " 'open',\n",
       " 'fort',\n",
       " 'mcmurray',\n",
       " 'help',\n",
       " 'residents',\n",
       " 'access',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " '1',\n",
       " '3',\n",
       " 'all',\n",
       " 'appointments',\n",
       " 'booked',\n",
       " 'ahs',\n",
       " 'fort',\n",
       " 'mcmurray',\n",
       " 'macdonald',\n",
       " 'island',\n",
       " 'park',\n",
       " 'curling',\n",
       " 'rink',\n",
       " 'april',\n",
       " '5',\n",
       " 'not',\n",
       " 'the',\n",
       " 'onion',\n",
       " 'column',\n",
       " 'california',\n",
       " 'rushed',\n",
       " 'vaccinate',\n",
       " 'poor',\n",
       " 'people',\n",
       " 'but',\n",
       " 'transgender',\n",
       " 'people',\n",
       " 'vaccinated',\n",
       " 'less',\n",
       " 'amount',\n",
       " 'people',\n",
       " 'died',\n",
       " 'due',\n",
       " 'covid',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'the',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'side',\n",
       " 'effects',\n",
       " 'expect',\n",
       " 'based',\n",
       " 'age',\n",
       " 'sex',\n",
       " 'dose',\n",
       " 'via',\n",
       " 'remember',\n",
       " 'they',\n",
       " 'are',\n",
       " 'the',\n",
       " 'people',\n",
       " 'who',\n",
       " 'bring',\n",
       " 'us',\n",
       " 'our',\n",
       " 'food',\n",
       " 'products',\n",
       " 'happy',\n",
       " 'biden',\n",
       " 'help',\n",
       " 'trump',\n",
       " 'refused',\n",
       " 'republicans',\n",
       " 'cruel',\n",
       " 'evil',\n",
       " 'heartless',\n",
       " 'every',\n",
       " 'rich',\n",
       " 'country',\n",
       " 'give',\n",
       " 'help',\n",
       " 'covid19',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'vaccines',\n",
       " 'finally',\n",
       " 'reaching',\n",
       " 'poor',\n",
       " 'countries',\n",
       " 'cover',\n",
       " 'cost',\n",
       " 'administering',\n",
       " 'black',\n",
       " 'folks',\n",
       " 'wanting',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'might',\n",
       " 'blessing',\n",
       " 'disguise',\n",
       " 'rest',\n",
       " 'us',\n",
       " 'registration',\n",
       " 'montgomery',\n",
       " 'county',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'clinics',\n",
       " 'ages',\n",
       " '16',\n",
       " 'opens',\n",
       " 'friday',\n",
       " 'about',\n",
       " '25',\n",
       " 'worcester',\n",
       " 'population',\n",
       " 'received',\n",
       " 'least',\n",
       " '1',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'numbers',\n",
       " 'still',\n",
       " 'hover',\n",
       " 'higher',\n",
       " 'officials',\n",
       " 'would',\n",
       " 'like',\n",
       " 'don',\n",
       " 'freak',\n",
       " 'get',\n",
       " 'side',\n",
       " 'effects',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'i',\n",
       " 'think',\n",
       " 'every',\n",
       " 'business',\n",
       " 'require',\n",
       " 'vaccine',\n",
       " 'proof',\n",
       " 'mask',\n",
       " 'wearing',\n",
       " 'absence',\n",
       " 'proof',\n",
       " 'fastest',\n",
       " 'way',\n",
       " 'get',\n",
       " 'covid',\n",
       " 'devastation',\n",
       " 'behind',\n",
       " 'us',\n",
       " 'economy',\n",
       " 'world',\n",
       " 'heal',\n",
       " 'move',\n",
       " 'gps',\n",
       " 'say',\n",
       " 'federal',\n",
       " 'government',\n",
       " 'set',\n",
       " 'us',\n",
       " 'fail',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'rollout',\n",
       " 'the',\n",
       " 'morrison',\n",
       " 'government',\n",
       " 'fails',\n",
       " 'big',\n",
       " 'actions',\n",
       " 'decisions',\n",
       " 'they',\n",
       " 'messed',\n",
       " 'covid19',\n",
       " 'vaccination',\n",
       " 'program',\n",
       " 'auspol',\n",
       " 'as',\n",
       " 'part',\n",
       " 'santa',\n",
       " 'barbara',\n",
       " 'county',\n",
       " 'health',\n",
       " 'center',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'program',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccines',\n",
       " 'available',\n",
       " '7',\n",
       " 'day',\n",
       " 'clinic',\n",
       " 'hancock',\n",
       " 'santa',\n",
       " 'maria',\n",
       " 'c',\n",
       " 'us',\n",
       " 'april',\n",
       " '5',\n",
       " '11',\n",
       " 'learn',\n",
       " 'eligible',\n",
       " 'make',\n",
       " 'appointment',\n",
       " 'today',\n",
       " 'getting',\n",
       " 'pfizer',\n",
       " 'vaccine',\n",
       " 'saturday',\n",
       " 'johnson',\n",
       " 'johnson',\n",
       " '66',\n",
       " 'covid',\n",
       " 'protein',\n",
       " 'maderna',\n",
       " '67',\n",
       " '2',\n",
       " 'shot',\n",
       " 'pfizer',\n",
       " '90',\n",
       " 'single',\n",
       " 'shot',\n",
       " 'covid',\n",
       " '19',\n",
       " 'vaccines',\n",
       " 'play',\n",
       " 'key',\n",
       " 'role',\n",
       " 'making',\n",
       " 'family',\n",
       " 'community',\n",
       " 'safer',\n",
       " 'find',\n",
       " 'facts',\n",
       " 'vaccines',\n",
       " 'file',\n",
       " 'reasons',\n",
       " 'i',\n",
       " 'glad',\n",
       " 'i',\n",
       " 'moved',\n",
       " 'seattle',\n",
       " 'i',\n",
       " 'heard',\n",
       " 'doctor',\n",
       " 'flu',\n",
       " 'jabs',\n",
       " 'incorporating',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'my',\n",
       " 'family',\n",
       " 'i',\n",
       " 'take',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'it',\n",
       " 'healthy',\n",
       " 'apostle',\n",
       " 'suleman',\n",
       " 'says',\n",
       " 'lailas',\n",
       " 'news',\n",
       " 'new',\n",
       " 'kaiser',\n",
       " 'permanente',\n",
       " 'panorama',\n",
       " 'medical',\n",
       " 'center',\n",
       " 'area',\n",
       " '05',\n",
       " '06',\n",
       " '05',\n",
       " '07',\n",
       " 'total',\n",
       " 'appointments',\n",
       " 'location',\n",
       " '35',\n",
       " 'address',\n",
       " '8250',\n",
       " 'woodman',\n",
       " 'ave',\n",
       " 'panorama',\n",
       " 'city',\n",
       " 'ca',\n",
       " '91402',\n",
       " 'posted',\n",
       " '1617314233',\n",
       " 'manon',\n",
       " 'its',\n",
       " '101',\n",
       " 'effective',\n",
       " 'oops',\n",
       " 'sorryyyy',\n",
       " 'just',\n",
       " 'got',\n",
       " 'second',\n",
       " 'dose',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'i',\n",
       " 'think',\n",
       " 'people',\n",
       " 'still',\n",
       " 'arguing',\n",
       " 'increase',\n",
       " 'vaccination',\n",
       " 'announce',\n",
       " 'lockdown',\n",
       " 'but',\n",
       " 'fact',\n",
       " 'even',\n",
       " 'vaccines',\n",
       " 'people',\n",
       " 'getting',\n",
       " 'infected',\n",
       " 'staying',\n",
       " 'home',\n",
       " 'avoiding',\n",
       " 'contact',\n",
       " 'solution',\n",
       " 'vaccine',\n",
       " 'ongoing',\n",
       " 'process',\n",
       " 'vaccine',\n",
       " 'covid',\n",
       " '19',\n",
       " 'mumbailockdown',\n",
       " 'anybody',\n",
       " 'interested',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'moderna',\n",
       " 'dm',\n",
       " 'u',\n",
       " 's',\n",
       " 'air',\n",
       " 'travel',\n",
       " 'vacation',\n",
       " 'spots',\n",
       " 'is',\n",
       " 'returning',\n",
       " 'i',\n",
       " 'getting',\n",
       " 'second',\n",
       " 'dose',\n",
       " 'covid',\n",
       " 'vaccine',\n",
       " 'tomorrow',\n",
       " 'i',\n",
       " 'try',\n",
       " 'remember',\n",
       " 'tweet',\n",
       " 'updates',\n",
       " 'symptoms',\n",
       " 'you',\n",
       " 'also',\n",
       " 'dm',\n",
       " 'questions',\n",
       " 'lots',\n",
       " 'people',\n",
       " 'arms',\n",
       " 'community',\n",
       " 'vaccination',\n",
       " 'rural',\n",
       " 'remote',\n",
       " 'areas',\n",
       " 'sensible',\n",
       " 'policy',\n",
       " 'sort',\n",
       " 'this',\n",
       " 'qld',\n",
       " 'health',\n",
       " 'vaccine',\n",
       " 'equity',\n",
       " 'race',\n",
       " 'inoculate',\n",
       " 'health',\n",
       " 'workers',\n",
       " 'risk',\n",
       " 'globally',\n",
       " 'covid',\n",
       " '19',\n",
       " 'un',\n",
       " 'news',\n",
       " 'the',\n",
       " 'clinic',\n",
       " 'closest',\n",
       " 'listed',\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency and bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T19:37:33.436229Z",
     "start_time": "2021-04-05T19:37:33.256105Z"
    }
   },
   "outputs": [],
   "source": [
    "words_freq = FreqDist(stopped_data)\n",
    "words_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T19:39:25.658169Z",
     "start_time": "2021-04-05T19:39:25.655999Z"
    }
   },
   "outputs": [],
   "source": [
    "#bigrams\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "tweets_finder = nltk.collocations.BigramCollocationFinder.from_words(stopped_data)\n",
    "tweets_scored = tweets_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "# show top 10 bigrams\n",
    "tweets_scored[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/tweets_sentiment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:02:15.948290Z",
     "start_time": "2021-04-05T21:02:15.904497Z"
    }
   },
   "outputs": [],
   "source": [
    "# use for now\n",
    "data = pd.read_csv('data/text_sentiment_TextBlob.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:02:22.350182Z",
     "start_time": "2021-04-05T21:02:22.346642Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tokenize(tweet):\n",
    "    tknzr = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "    return tknzr.tokenize(tweet)\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:16:20.606317Z",
     "start_time": "2021-04-05T21:16:20.604038Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(stop_words= 'english', tokenizer= tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:20:04.417428Z",
     "start_time": "2021-04-05T21:20:04.411330Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data['tweets'].astype('U')\n",
    "# y = data['Google Sentiment']\n",
    "y = data['TB Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:20:05.186821Z",
     "start_time": "2021-04-05T21:20:05.178128Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:20:08.839143Z",
     "start_time": "2021-04-05T21:20:06.068330Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_Xtrain = tfidf_vect.fit_transform(X_train)\n",
    "tfidf_Xtest = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:20:12.147288Z",
     "start_time": "2021-04-05T21:20:12.143757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14860x19857 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 210267 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:20:19.482367Z",
     "start_time": "2021-04-05T21:20:19.478648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14860,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3715,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:20:39.696937Z",
     "start_time": "2021-04-05T21:20:39.693222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3715, 19857)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_Xtest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:56:19.337954Z",
     "start_time": "2021-04-05T21:56:19.257899Z"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary libraries for modeling\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report,\\\n",
    "confusion_matrix, roc_auc_score, plot_confusion_matrix, plot_roc_curve\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:56:34.955803Z",
     "start_time": "2021-04-05T21:56:34.952790Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate classifiers for vanilla models\n",
    "classifiers = {\n",
    "    'Logistic Regression' : LogisticRegression(), \n",
    "    'Random Forest' : RandomForestClassifier(), \n",
    "    'Support Vector Machine' : SVC(), \n",
    "    'XGBoost' : XGBClassifier(), \n",
    "    'Naive Bayes' : MultinomialNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T22:05:14.367188Z",
     "start_time": "2021-04-05T22:05:14.333011Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Johnson Johnson to begin testing COVID 19 vaccine on younger teens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-294342692e41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# fit models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1340\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[1;32m   1343\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    766\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    767\u001b[0m         \"\"\"\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Johnson Johnson to begin testing COVID 19 vaccine on younger teens'"
     ]
    }
   ],
   "source": [
    "# create DataFrame to hold results from vanilla models\n",
    "# takes approx 2 mins to run\n",
    "results = pd.DataFrame(columns= ['Train_accuracy', 'Test_accuracy', 'F1_score'])\n",
    "\n",
    "# predict, get accuracy and f1 scores and add to dataframe\n",
    "conf_matrices = []\n",
    "for key, value in classifiers.items():\n",
    "    # fit models\n",
    "    value.fit(X_train, y_train)\n",
    "    train_pred = value.predict(X_train)\n",
    "    y_pred = value.predict(X_test)\n",
    "    \n",
    "    # get accuracy, f1 score\n",
    "    train_acc = accuracy_score(y_train, train_pred) * 100\n",
    "    test_acc = accuracy_score(y_test, y_pred) * 100\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # get false neg and true positive\n",
    "    # add to confusion matrices list to view later if desired\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    conf_matrices.append(cm)\n",
    "    \n",
    "    # add measurements to datafram\n",
    "    results.loc[key] = [round(train_acc, 2), round(test_acc, 2), \n",
    "                        round(f1, 2), round(FN, 0), round(TP, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "276px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
