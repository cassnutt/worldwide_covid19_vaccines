{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "\n",
    "This notebook aims to analyze Tweets made about the COVID-19 vaccine and determine whether the sentiment of the Tweets is negative, positive, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:52:04.421315Z",
     "start_time": "2021-04-05T16:52:03.126474Z"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import webbrowser\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import time\n",
    "\n",
    "import config\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "\n",
    "import os\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import translate_v2 as translate\n",
    "from google.cloud import language\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "import string\n",
    "\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather data from Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:51.118839Z",
     "start_time": "2021-04-05T15:24:51.114141Z"
    }
   },
   "outputs": [],
   "source": [
    "# Developer API keys\n",
    "# API keys in config.py in .gitignore\n",
    "callback_uri = 'oob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:51.123967Z",
     "start_time": "2021-04-05T15:24:51.121171Z"
    }
   },
   "outputs": [],
   "source": [
    "auth = OAuthHandler(config.consumer_key, config.consumer_secret, callback_uri)\n",
    "auth.set_access_token(config.access_token, config.access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:51.128241Z",
     "start_time": "2021-04-05T15:24:51.125428Z"
    }
   },
   "outputs": [],
   "source": [
    "api = API(auth, wait_on_rate_limit= True, wait_on_rate_limit_notify= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:51.535816Z",
     "start_time": "2021-04-05T15:24:51.130182Z"
    }
   },
   "outputs": [],
   "source": [
    "# show my Twitter handle\n",
    "me = api.me()\n",
    "print(me.screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:51.936887Z",
     "start_time": "2021-04-05T15:24:51.537868Z"
    }
   },
   "outputs": [],
   "source": [
    "# simple query to get the hang of it\n",
    "query = '#vaccine covid -RT -@ -http'\n",
    "for i, status in enumerate(tweepy.Cursor(api.search, q= query).items(15)):\n",
    "    print (i, status.text)\n",
    "\n",
    "# filter only English tweets    \n",
    "# query = '#vaccine covid'\n",
    "# for i, status in enumerate(tweepy.Cursor(api.search, q= query, lang= 'en').items(50)):\n",
    "#     print (i, status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:51.946569Z",
     "start_time": "2021-04-05T15:24:51.941271Z"
    }
   },
   "outputs": [],
   "source": [
    "''' This cell was run to collect the initial 5000 tweets and save them to a csv file\n",
    "is commented out to prevent it from being run again\n",
    "\n",
    "\n",
    "# collect 5000 tweets \n",
    "number_of_tweets = 5000\n",
    "tweets = []\n",
    "language = []\n",
    "time = []\n",
    "\n",
    "# only collect full text, language, and time it was posted\n",
    "# remove any retweets '-RT'\n",
    "for i in tweepy.Cursor(api.search, q= 'vaccine covid -RT',  \n",
    "                       tweet_mode = 'extended').items(number_of_tweets):\n",
    "    tweets.append(i.full_text)\n",
    "    language.append(i.lang)\n",
    "    time.append(i.created_at)\n",
    "\n",
    "# turn tweets / lists into dataframe\n",
    "df = pd.DataFrame({'time': time, 'language': language, 'tweets': tweets})\n",
    "\n",
    "# save df to csv\n",
    "df.to_csv('data/tweets.csv', index= False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect more tweets after waiting to avoid collecting the same tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:51.952904Z",
     "start_time": "2021-04-05T15:24:51.949278Z"
    }
   },
   "outputs": [],
   "source": [
    "# create function to get more tweets and add to existing csv\n",
    "def get_more_tweets(num_tweets):\n",
    "    number_of_tweets = num_tweets\n",
    "    tweets = []\n",
    "    language = []\n",
    "    time = []\n",
    "\n",
    "    # only collect full text, language, and time it was posted\n",
    "    # remove any retweets '-RT'\n",
    "    for i in tweepy.Cursor(api.search, q= 'vaccine covid -RT',  \n",
    "                           tweet_mode = 'extended').items(number_of_tweets):\n",
    "        tweets.append(i.full_text)\n",
    "        language.append(i.lang)\n",
    "        time.append(i.created_at)\n",
    "        \n",
    "    # turn tweets / lists into dataframe\n",
    "    df = pd.DataFrame({'time': time, 'language': language, 'tweets': tweets})\n",
    "    \n",
    "    # add to existing csv file\n",
    "    df.to_csv('data/tweets.csv', mode= 'a', index= False, header= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:51.956541Z",
     "start_time": "2021-04-05T15:24:51.954673Z"
    }
   },
   "outputs": [],
   "source": [
    "# get tweets and save to csv through function\n",
    "\n",
    "# n_tweets = 20\n",
    "# get_more_tweets(n_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T14:14:15.835034Z",
     "start_time": "2021-04-01T14:14:15.833105Z"
    }
   },
   "source": [
    "### Search for Tweets from 30 Days Ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:51.961936Z",
     "start_time": "2021-04-05T15:24:51.958033Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to turn results into DataFrame\n",
    "def extract_text_as_df(text_list):\n",
    "    columns = set()\n",
    "    allowed_types = [str, int]\n",
    "    tweets_data = []\n",
    "    for status in text_list:\n",
    "        status_dict = dict(vars(status))\n",
    "        keys = status_dict.keys()\n",
    "        single_tweet_data = {\"author\": status.author.screen_name}\n",
    "        for k in keys:\n",
    "            try:\n",
    "                v_type = type(status_dict[k])\n",
    "            except:\n",
    "                v_type = None\n",
    "            if v_type != None:\n",
    "                if v_type in allowed_types:\n",
    "                    single_tweet_data[k] = status_dict[k]\n",
    "                    columns.add(k)\n",
    "        tweets_data.append(single_tweet_data)\n",
    "\n",
    "\n",
    "    header_cols = list(columns)\n",
    "    header_cols.append('author')\n",
    "    df = pd.DataFrame(tweets_data, columns=header_cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:52.597337Z",
     "start_time": "2021-04-05T15:24:51.964023Z"
    }
   },
   "outputs": [],
   "source": [
    "query = 'vaccine covid -RT'\n",
    "results_30 = api.search_30_day('datacollection', query, maxResults= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:52.604020Z",
     "start_time": "2021-04-05T15:24:52.598796Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "old_results_df = extract_text_as_df(results_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:52.631612Z",
     "start_time": "2021-04-05T15:24:52.605788Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "old_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:52.697342Z",
     "start_time": "2021-04-05T15:24:52.633749Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:52.710510Z",
     "start_time": "2021-04-05T15:24:52.699000Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:52.732939Z",
     "start_time": "2021-04-05T15:24:52.712318Z"
    }
   },
   "outputs": [],
   "source": [
    "# check for duplicated tweets\n",
    "duped = df[df.duplicated(subset= 'tweets')]\n",
    "duped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:52.755780Z",
     "start_time": "2021-04-05T15:24:52.734718Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df.drop_duplicates(subset= 'tweets', keep= 'first', inplace= True)\n",
    "df.reset_index(drop= True, inplace= True)\n",
    "\n",
    "# make sure they are gone\n",
    "df.tweets.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:52.767840Z",
     "start_time": "2021-04-05T15:24:52.757712Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:52.783491Z",
     "start_time": "2021-04-05T15:24:52.769738Z"
    }
   },
   "outputs": [],
   "source": [
    "lang_df = df.sort_values('language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:53.210023Z",
     "start_time": "2021-04-05T15:24:52.785619Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (12, 8))\n",
    "lang_df.language.value_counts(normalize=True).plot(kind= 'barh')\n",
    "plt.title('Proportion of Languages Collected')\n",
    "plt.xlim(-.01, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:53.218806Z",
     "start_time": "2021-04-05T15:24:53.211880Z"
    }
   },
   "outputs": [],
   "source": [
    "lang_df.language.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:53.643072Z",
     "start_time": "2021-04-05T15:24:53.224721Z"
    }
   },
   "outputs": [],
   "source": [
    "# what are these languages? get table from url\n",
    "URL = 'https://developer.twitter.com/en/docs/twitter-for-websites/supported-languages'\n",
    "tables = pd.read_html(URL)\n",
    "print(\"There are : \",len(tables),\" tables\")\n",
    "print(tables[0])\n",
    "\n",
    "lang_table = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:53.651902Z",
     "start_time": "2021-04-05T15:24:53.647679Z"
    }
   },
   "outputs": [],
   "source": [
    "# format table\n",
    "header_row = 0\n",
    "lang_table.columns = lang_table.iloc[header_row]\n",
    "lang_table = lang_table.drop(header_row)\n",
    "lang_table.reset_index(drop= True, inplace= True)\n",
    "\n",
    "lang_table.replace('English (default)', 'English', inplace= True)\n",
    "# lang_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:53.658658Z",
     "start_time": "2021-04-05T15:24:53.653693Z"
    }
   },
   "outputs": [],
   "source": [
    "# save as csv in data folder\n",
    "lang_table.to_csv('data/languages.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:53.669643Z",
     "start_time": "2021-04-05T15:24:53.660263Z"
    }
   },
   "outputs": [],
   "source": [
    "# look at 'und' language \n",
    "df[df['language'] == 'und']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the 'und' or possibly 'undetermined' language are tweets that do not include text to help us with our sentiment analysis. We will remove those from our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:53.676346Z",
     "start_time": "2021-04-05T15:24:53.671035Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['language'] != 'und']\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the data with the languages to see the whole name of languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:53.698843Z",
     "start_time": "2021-04-05T15:24:53.678271Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge data with language names\n",
    "df2 = df.merge(lang_table, how= 'inner', left_on= 'language', right_on= 'Language code')\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:53.715853Z",
     "start_time": "2021-04-05T15:24:53.700544Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop extra columns and rename them and change dates to datetime and drop time\n",
    "df2.drop(columns = ['language', 'Language code'], inplace= True)\n",
    "df2.rename(columns={'Name': 'language'}, inplace= True)\n",
    "df2['time'] = pd.to_datetime(df2['time']).dt.date\n",
    "# df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many Tweets are in each language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:53.986600Z",
     "start_time": "2021-04-05T15:24:53.717541Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(y= 'language', data= df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of Tweets are in English. Let's see what the other languages are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:54.223545Z",
     "start_time": "2021-04-05T15:24:53.988733Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# countplot that excludes English to see detail\n",
    "plt.figure(figsize= (10, 8))\n",
    "sns.countplot(y= 'language', data= df2, palette='plasma',\n",
    "              order= df2.language.value_counts().iloc[1:].index)\n",
    "plt.title('Count of Tweets, Excluding English', fontsize= 14)\n",
    "plt.xlabel('# of Tweets')\n",
    "plt.ylabel('Language')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After English, the top 3 languages for Tweets about the COVID-19 vaccine are in French, Spanish, and Japanese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:54.232574Z",
     "start_time": "2021-04-05T15:24:54.225152Z"
    }
   },
   "outputs": [],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate to English using Google Cloud Platform's API\n",
    "\n",
    "**Most of this section has been commented out to prevent running on Google Cloud.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:19:32.791585Z",
     "start_time": "2021-04-05T16:19:32.784347Z"
    }
   },
   "outputs": [],
   "source": [
    "# send in credentials that are saved on local computer\n",
    "credentials = service_account.Credentials.from_service_account_file('../GCP_service_account.json')\n",
    "translate_client = translate.Client(credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:19:42.695015Z",
     "start_time": "2021-04-05T16:19:42.470436Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see all languages that Google can translate\n",
    "print(translate_client.get_languages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:54.667586Z",
     "start_time": "2021-04-05T15:24:49.608Z"
    }
   },
   "outputs": [],
   "source": [
    "# separate non-English tweets for faster translating\n",
    "part_df = df2.loc[16100:]\n",
    "# part_df.language.value_counts(normalize= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:54.668941Z",
     "start_time": "2021-04-05T15:24:49.610Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dictionary to save original tweet and translated one\n",
    "# commented out to prevent re-running on Google Cloud\n",
    "'''\n",
    "results = {}\n",
    "\n",
    "# translate tweets into English\n",
    "for i in part_df.tweets.loc[:]:\n",
    "    result = translate_client.translate(i, target_language= 'en')\n",
    "    results[i] = result['translatedText']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:54.670320Z",
     "start_time": "2021-04-05T15:24:49.612Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace with English translation\n",
    "# df2.tweets.replace(results, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:54.672034Z",
     "start_time": "2021-04-05T15:24:49.613Z"
    }
   },
   "outputs": [],
   "source": [
    "# preview previously French Tweets\n",
    "# df2[df2['language'] == 'French']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:54.673450Z",
     "start_time": "2021-04-05T15:24:49.615Z"
    }
   },
   "outputs": [],
   "source": [
    "# save to csv file\n",
    "# df2.to_csv('data/tweets_translated.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Sentiment\n",
    "---\n",
    "Does the tweet have a positive, negative, or neutral tone?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering for Text Data\n",
    "--- \n",
    "stopword removal\n",
    "frequency distributions\n",
    "lemmatization\n",
    "bigramsm n-grams, and mutal information score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:52:20.599151Z",
     "start_time": "2021-04-05T16:52:20.536005Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data from csv\n",
    "df2 = pd.read_csv('data/tweets_translated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:52:21.781571Z",
     "start_time": "2021-04-05T16:52:21.776905Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove language column\n",
    "sentiment_df = df2[['time', 'tweets']]\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Platform Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:52:26.842065Z",
     "start_time": "2021-04-05T16:52:26.834986Z"
    }
   },
   "outputs": [],
   "source": [
    "# input credentials from local file to access Google Cloud Platform\n",
    "credentials = service_account.Credentials.from_service_account_file('../GCP_service_account.json')\n",
    "nlp_client = language.Client(credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T17:15:38.439484Z",
     "start_time": "2021-04-05T17:15:38.436163Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to collect sentiment from Google Cloud\n",
    "scores = []\n",
    "magnitudes = []\n",
    "\n",
    "def getGoogleSentiments(txt):\n",
    "    document = nlp_client.document_from_text(txt)\n",
    "    \n",
    "    sent_analysis = document.analyze_sentiment()\n",
    "    sentiment = sent_analysis.sentiment\n",
    "    \n",
    "    # get score and magnitude and add to list\n",
    "    score = sentiment.score\n",
    "    scores.append(score)\n",
    "    magnitude = sentiment.magnitude\n",
    "    magnitudes.append(magnitude)\n",
    "    # add delay to prevent hitting rate limit\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-05T17:16:14.101Z"
    }
   },
   "outputs": [],
   "source": [
    "# get sentiment score and magnitude from function\n",
    "google_sent = sentiment_df.tweets.apply(getGoogleSentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Google Cloud's Natural Language sentiment analysis markers:_\n",
    "\n",
    "**Score** of the sentiment ranges between -1.0 (negative) and 1.0 (positive) and corresponds to the overall emotional leaning of the text.\n",
    "\n",
    "**Magnitude** indicates the overall strength of emotion (both positive and negative) within the given text, between 0.0 and +inf. Unlike score, magnitude is not normalized; each expression of emotion within the text (both positive and negative) contributes to the text's magnitude (so longer text blocks may have greater magnitudes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lists to DataFrame as new columns\n",
    "sentiment_df['Google Score'] = scores\n",
    "sentiment_df['Google Magnitude'] = magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine if Tweet was positive, negative, or neutral\n",
    "def getTextAnalysis(p):\n",
    "    if p < 0:\n",
    "        return 'Negative'\n",
    "    elif p == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df['Google Sentiment'] = sentiment_df['Google Score'].apply(getTextAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot as bar graph\n",
    "plt.figure(figsize= (10, 6))\n",
    "sns.countplot(data= sentiment_df, x = 'Google Sentiment')\n",
    "plt.title('Sentiment Analysis of Tweets about COVID-19 Vaccine', fontsize = 14)\n",
    "plt.xlabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot polarity and subjectivity and sentiment of the Tweet\n",
    "plt.figure(figsize= (10,8))\n",
    "sns.scatterplot(x= 'Google Score', y= 'Google Magnitude', data= sentiment_df, hue= 'Google Sentiment')\n",
    "plt.title('Polarity and Subjectivity of COVID-19 Vaccine Tweets by Sentiment', \n",
    "          fontsize = 14, fontweight= 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv file\n",
    "# sentiment_df.to_csv('data/tweets_google_sentiment.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:26:40.627007Z",
     "start_time": "2021-04-05T16:26:40.624316Z"
    }
   },
   "outputs": [],
   "source": [
    "sent1 = sentiment_df[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:27:47.342917Z",
     "start_time": "2021-04-05T16:26:58.656682Z"
    }
   },
   "outputs": [],
   "source": [
    "google_sent = sent1.tweets.apply(getGoogleSentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:28:10.130801Z",
     "start_time": "2021-04-05T16:28:10.119178Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:34:16.676804Z",
     "start_time": "2021-04-05T16:34:16.673521Z"
    }
   },
   "outputs": [],
   "source": [
    "def getMoreGoogleSentiments(txt):\n",
    "    document = nlp_client.document_from_text(txt)\n",
    "    \n",
    "    sent_analysis = document.analyze_sentiment()\n",
    "    sentiment = sent_analysis.sentiment\n",
    "    \n",
    "    score = sentiment.score\n",
    "    scores.extendnd(score)\n",
    "    magnitude = sentiment.magnitude\n",
    "    magnitudes.extend(magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:33:46.747751Z",
     "start_time": "2021-04-05T16:33:46.745074Z"
    }
   },
   "outputs": [],
   "source": [
    "sent2 = sentiment_df[500:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_sent = sent2.tweets.apply(getMoreGoogleSentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.read_csv('data/tweets_google_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:54.684636Z",
     "start_time": "2021-04-05T15:24:49.632Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions to clean tweets, get subjectivity and polarity scores and sentiment\n",
    "def cleanTweet(txt):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|(amp)\", \n",
    "                           \" \", txt).split())\n",
    "\n",
    "def getTextSubjectivity(txt):\n",
    "    '''Subjective sentences generally refer to personal opinion, emotion \n",
    "    or judgment whereas objective refers to factual information. \n",
    "    Subjectivity is also a float which lies in the range of [0,1].'''\n",
    "    return TextBlob(txt).sentiment.subjectivity\n",
    "\n",
    "def getTextPolarity(txt):\n",
    "    '''Polarity is float which lies in the range of [-1,1] \n",
    "    where 1 means positive statement and -1 means a negative statement.'''\n",
    "    return TextBlob(txt).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:54.685910Z",
     "start_time": "2021-04-05T15:24:49.634Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply functions to tweets\n",
    "sentiment_df['tweets'] = sentiment_df['tweets'].apply(clean_tweet)\n",
    "sentiment_df['TB Subjectivity'] = sentiment_df['tweets'].apply(getTextSubjectivity)\n",
    "sentiment_df['TB Polarity'] = sentiment_df['tweets'].apply(getTextPolarity)\n",
    "sentiment_df['TB Sentiment'] = sentiment_df['TB Polarity'].apply(getTextAnalysis)\n",
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:54.687082Z",
     "start_time": "2021-04-05T15:24:49.636Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot as bar graph\n",
    "plt.figure(figsize= (10, 6))\n",
    "sns.countplot(data= sentiment_df, x = 'TB Sentiment')\n",
    "plt.title('Sentiment Analysis of Tweets about COVID-19 Vaccine', fontsize = 14)\n",
    "plt.xlabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:54.687909Z",
     "start_time": "2021-04-05T15:24:49.637Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot polarity and subjectivity and sentiment of the Tweet\n",
    "plt.figure(figsize= (10,8))\n",
    "sns.scatterplot(x= 'TB Polarity', y= 'TB Subjectivity', data= sentiment_df, hue= 'TB Sentiment')\n",
    "plt.title('Polarity and Subjectivity of COVID-19 Vaccine Tweets by Sentiment', \n",
    "          fontsize = 14, fontweight= 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all to csv\n",
    "# sentiment_df.to_csv('data/tweets_sentiment_data.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud, Word Freqency and Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:24:54.689134Z",
     "start_time": "2021-04-05T15:24:49.642Z"
    }
   },
   "outputs": [],
   "source": [
    "# create word cloud\n",
    "words = ' '.join([tweet for tweet in sentiment_df['tweets']])\n",
    "wc = WordCloud(background_color= '#222222').generate(words)\n",
    "plt.figure(figsize= (10, 6))\n",
    "plt.imshow(wc.recolor(colormap= 'spring'))\n",
    "plt.axis('off')\n",
    "plt.title('COVID-19 Vaccine Word Cloud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing\n",
    "tokens = dataf['tweets'].map(word_tokenize)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # remove stop words make make all words lowercase\n",
    "stopped_data = [w.lower() for l in tokens for w in l if w not in stopwords_list]\n",
    "print(len(stopped_data))\n",
    "stopped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show 20 most common words\n",
    "words_freq = FreqDist(stopped_data)\n",
    "freq_words_20 = words_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 20 most frequent words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "tweets_finder = nltk.collocations.BigramCollocationFinder.from_words(stopped_data)\n",
    "\n",
    "tweets_scored = tweets_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "tweets_scored[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences from Google and TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Modeling\n",
    "---\n",
    "To build models of our tweets, we first need to do some more cleaning that Google and TextBlob didn't require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize, TweetTokenizer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re \n",
    "from textblob import TextBlob\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "import string\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "282.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
